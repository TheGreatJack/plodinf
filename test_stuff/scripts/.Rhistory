probability_table <- data.frame()
for (read_depth in c(2:max_read_depth)){
for (peak_prob_name in names(peak_probs_list)){
data_r <-restricted_freq_distr_viable(read_depth, peak_probs_list[[peak_prob_name]])
data_r$peak_type <- peak_prob_name
data_r$read_depth <- read_depth
probability_table <- rbind(probability_table, data_r)
}
}
return(probability_table)
}
restricted_freq_distr_viable <- function(read_depth, peak_probs){
reads_a <- seq(1,read_depth-1,1)
reads_b <- seq(read_depth-1,1,-1)
# Strong future optimization of this double binom estimation
read_binom_probs_a <- dbinom(reads_a,size = read_depth, prob = peak_probs[1])
read_binom_probs_b <- dbinom(reads_b,size = read_depth, prob = peak_probs[2])
final_freqs <- (reads_a - reads_b) / read_depth
final_freqs_probs <- (read_binom_probs_a + read_binom_probs_b ) / (2*sum(read_binom_probs_a))
final_df <- data.frame(freqs = final_freqs, probs = final_freqs_probs)
return(final_df)
}
compute_pmf_probs <- function(base_prob_table, read_depth_distr, central_prop){
read_depth_table <- as.data.frame(table(read_depth_distr))
out_ward_prop <- (1-central_prop)
filtered_base_table <- base_prob_table[base_prob_table$read_depth %in% read_depth_table$read_depth_distr,]
filtered_base_table[filtered_base_table$peak_type == "0.5_0.5",c("probs")] <- filtered_base_table[filtered_base_table$peak_type == "0.5_0.5",c("probs")] * central_prop
filtered_base_table[filtered_base_table$peak_type == "0.25_0.75",c("probs")] <- filtered_base_table[filtered_base_table$peak_type == "0.25_0.75",c("probs")] * out_ward_prop
final_weights <- aggregate(probs ~ freqs, filtered_base_table, FUN = sum)
# I rest 1 at the end because read depths of 1 get ignored.....
final_weights$probs <- final_weights$probs / (length(read_depth_table$read_depth_distr) - 1)
return(final_weights)
}
simple_pmf_function <- function(x, final_pmf_weights){
if (x %in% final_pmf_weights$freqs){
return(final_pmf_weights[final_pmf_weights$freqs == x, c("probs")])
}else{
return(0)
}
}
simple_cdf_func <- function(x,weights) {
# Check if weights is a data frame with columns 'freqs' and 'probs'
if (!all(c("freqs", "probs") %in% names(weights))) {
stop("weights must be a data frame with columns 'freqs' and 'probs'")
}
# Sort weights by 'freqs' in ascending order
weights <- weights[order(weights$freqs), ]
cdf_probs <-c()
for (val in x){
#print(val)
# Initialize cumulative probability
cdf <- 0
# Loop through weights and calculate cumulative sum
for (i in 1:nrow(weights)) {
if (weights$freqs[i] <= val) {
cdf <- cdf + weights$probs[i]
} else {
cdf_probs <- c(cdf_probs,cdf)
break  # Stop iterating when freqs exceed x
}
}
#print("Precheck")
#print(weights$freqs[-1])
if (val >= tail(weights$freqs,n=1)){
cdf_probs <- c(cdf_probs,1)
}
}
return(cdf_probs)
}
max_read_depth <- 500
peak_probs_list <- list("0.25_0.75"= c(0.25,0.75),"0.5_0.5" = c(0.5,0.5))
base_probability_test <- compute_base_probability_table(max_read_depth,peak_probs_list)
read_depth_distr <- ceiling(rexp(1000,1/4))
central_prop <- 0.3
final_pmf_weights_test <- compute_pmf_probs(base_probability_test, read_depth_distr, central_prop)
sum(final_pmf_weights_test$probs)
read_depth_distr <- ceiling(rexp(1000,1/4))
#read_depth_distr <- round(rnorm(1000,30,2))
central_prop <- 0.3
final_pmf_weights_test <- compute_pmf_probs(base_probability_test, read_depth_distr, central_prop)
library(ggplot2)
# Plot the distributions
ggplot(final_pmf_weights_test, aes(x = freqs, y = probs)) +
geom_point() +  # Line for first distribution
labs(title = "Binomial Distributions",
x = "Number of Successes",
y = "Probability") +
xlim(-1,1) +
ylim(0,0.3)
compute_base_probability_table <- function(max_read_depth,peak_probs_list){
probability_table <- data.frame()
for (read_depth in c(2:max_read_depth)){
for (peak_prob_name in names(peak_probs_list)){
data_r <-restricted_freq_distr_viable(read_depth, peak_probs_list[[peak_prob_name]])
data_r$peak_type <- peak_prob_name
data_r$read_depth <- read_depth
probability_table <- rbind(probability_table, data_r)
}
}
return(probability_table)
}
restricted_freq_distr_viable <- function(read_depth, peak_probs){
reads_a <- seq(1,read_depth-1,1)
reads_b <- seq(read_depth-1,1,-1)
# Strong future optimization of this double binom estimation
read_binom_probs_a <- dbinom(reads_a,size = read_depth, prob = peak_probs[1])
read_binom_probs_b <- dbinom(reads_a,size = read_depth, prob = peak_probs[2])
final_freqs <- (reads_a - reads_b) / read_depth
final_freqs_probs <- (read_binom_probs_a + read_binom_probs_b ) / (2*sum(read_binom_probs_a))
final_df <- data.frame(freqs = final_freqs, probs = final_freqs_probs)
return(final_df)
}
compute_pmf_probs <- function(base_prob_table, read_depth_distr, central_prop){
read_depth_table <- as.data.frame(table(read_depth_distr))
out_ward_prop <- (1-central_prop)
filtered_base_table <- base_prob_table[base_prob_table$read_depth %in% read_depth_table$read_depth_distr,]
filtered_base_table[filtered_base_table$peak_type == "0.5_0.5",c("probs")] <- filtered_base_table[filtered_base_table$peak_type == "0.5_0.5",c("probs")] * central_prop
filtered_base_table[filtered_base_table$peak_type == "0.25_0.75",c("probs")] <- filtered_base_table[filtered_base_table$peak_type == "0.25_0.75",c("probs")] * out_ward_prop
final_weights <- aggregate(probs ~ freqs, filtered_base_table, FUN = sum)
# I rest 1 at the end because read depths of 1 get ignored.....
final_weights$probs <- final_weights$probs / (length(read_depth_table$read_depth_distr) - 1)
return(final_weights)
}
simple_pmf_function <- function(x, final_pmf_weights){
if (x %in% final_pmf_weights$freqs){
return(final_pmf_weights[final_pmf_weights$freqs == x, c("probs")])
}else{
return(0)
}
}
simple_cdf_func <- function(x,weights) {
# Check if weights is a data frame with columns 'freqs' and 'probs'
if (!all(c("freqs", "probs") %in% names(weights))) {
stop("weights must be a data frame with columns 'freqs' and 'probs'")
}
# Sort weights by 'freqs' in ascending order
weights <- weights[order(weights$freqs), ]
cdf_probs <-c()
for (val in x){
#print(val)
# Initialize cumulative probability
cdf <- 0
# Loop through weights and calculate cumulative sum
for (i in 1:nrow(weights)) {
if (weights$freqs[i] <= val) {
cdf <- cdf + weights$probs[i]
} else {
cdf_probs <- c(cdf_probs,cdf)
break  # Stop iterating when freqs exceed x
}
}
#print("Precheck")
#print(weights$freqs[-1])
if (val >= tail(weights$freqs,n=1)){
cdf_probs <- c(cdf_probs,1)
}
}
return(cdf_probs)
}
read_depth_distr <- ceiling(rexp(1000,1/4))
#read_depth_distr <- round(rnorm(1000,30,2))
central_prop <- 0.3
final_pmf_weights_test <- compute_pmf_probs(base_probability_test, read_depth_distr, central_prop)
library(ggplot2)
# Plot the distributions
ggplot(final_pmf_weights_test, aes(x = freqs, y = probs)) +
geom_point() +  # Line for first distribution
labs(title = "Binomial Distributions",
x = "Number of Successes",
y = "Probability") +
xlim(-1,1) +
ylim(0,0.3)
read_depth <- 10
peak_probs <- c(0.5,0.5)
reads_a <- seq(0,read_depth,1)
reads_b <- seq(read_depth,0,-1)
# Strong future optimization of this double binom estimation
read_binom_probs_a <- dbinom(reads_a,size = read_depth, prob = peak_probs[1])
read_binom_probs_a
read_binom_probs_b <- dbinom(reads_b,size = read_depth, prob = peak_probs[2])
read_binom_probs_b
read_depth <- 10
peak_probs <- c(0.25,0.75)
reads_a <- seq(0,read_depth,1)
reads_b <- seq(read_depth,0,-1)
# Strong future optimization of this double binom estimation
read_binom_probs_a <- dbinom(reads_a,size = read_depth, prob = peak_probs[1])
read_binom_probs_b <- dbinom(reads_b,size = read_depth, prob = peak_probs[2])
read_binom_probs_a
read_binom_probs_b
final_freqs <- (reads_a - reads_b) / read_depth
# Strong future optimization of this double binom estimation
read_binom_probs_a <- dbinom(reads_a,size = read_depth, prob = peak_probs[1])
read_binom_probs_b <- dbinom(reads_b,size = read_depth, prob = peak_probs[2])
read_binom_probs_b
read_binom_probs_a
final_freqs <- (reads_a - reads_b) / read_depth
final_freqs_probs <- ( read_binom_probs_a + read_binom_probs_b ) / 2
final_df <- data.frame(freqs = final_freqs, probs = final_freqs_probs)
View(final_df)
dbinom(c(0,1),10,prob = 0.1)
dbinom(c(0,1),10,prob = 0.9)
dbinom(c(1,2),10,prob = 0.9)
dbinom(c(2,1),10,prob = 0.9)
reads_b <- seq(read_depth,0,-1)
reads_b
read_depth <- 10
peak_probs <- c(0.25,0.75)
reads_a <- seq(0,read_depth,1)
reads_b <- seq(read_depth,0,-1)
# Strong future optimization of this double binom estimation
read_binom_probs_a <- dbinom(reads_a,size = read_depth, prob = peak_probs[1])
read_binom_probs_b <- dbinom(reads_a,size = read_depth, prob = peak_probs[2])
final_freqs <- (reads_a - reads_b) / read_depth
final_freqs_probs <- ( read_binom_probs_a + read_binom_probs_b ) / 2
final_df <- data.frame(freqs = final_freqs, probs = final_freqs_probs)
compute_base_probability_table <- function(max_read_depth,peak_probs_list){
probability_table <- data.frame()
for (read_depth in c(2:max_read_depth)){
for (peak_prob_name in names(peak_probs_list)){
data_r <-restricted_freq_distr_viable(read_depth, peak_probs_list[[peak_prob_name]])
data_r$peak_type <- peak_prob_name
data_r$read_depth <- read_depth
probability_table <- rbind(probability_table, data_r)
}
}
return(probability_table)
}
restricted_freq_distr_viable <- function(read_depth, peak_probs){
reads_a <- seq(1,read_depth-1,1)
reads_b <- seq(read_depth-1,1,-1)
# Strong future optimization of this double binom estimation
read_binom_probs_a <- dbinom(reads_a,size = read_depth, prob = peak_probs[1])
read_binom_probs_b <- dbinom(reads_a,size = read_depth, prob = peak_probs[2])
final_freqs <- (reads_a - reads_b) / read_depth
final_freqs_probs <- (read_binom_probs_a + read_binom_probs_b ) / (2*sum(read_binom_probs_a))
final_df <- data.frame(freqs = final_freqs, probs = final_freqs_probs)
return(final_df)
}
compute_pmf_probs <- function(base_prob_table, read_depth_distr, central_prop){
read_depth_table <- as.data.frame(table(read_depth_distr))
out_ward_prop <- (1-central_prop)
filtered_base_table <- base_prob_table[base_prob_table$read_depth %in% read_depth_table$read_depth_distr,]
filtered_base_table[filtered_base_table$peak_type == "0.5_0.5",c("probs")] <- filtered_base_table[filtered_base_table$peak_type == "0.5_0.5",c("probs")] * central_prop
filtered_base_table[filtered_base_table$peak_type == "0.25_0.75",c("probs")] <- filtered_base_table[filtered_base_table$peak_type == "0.25_0.75",c("probs")] * out_ward_prop
final_weights <- aggregate(probs ~ freqs, filtered_base_table, FUN = sum)
# I rest 1 at the end because read depths of 1 get ignored.....
final_weights$probs <- final_weights$probs / (length(read_depth_table$read_depth_distr) - 1)
return(final_weights)
}
simple_pmf_function <- function(x, final_pmf_weights){
if (x %in% final_pmf_weights$freqs){
return(final_pmf_weights[final_pmf_weights$freqs == x, c("probs")])
}else{
return(0)
}
}
simple_cdf_func <- function(x,weights) {
# Check if weights is a data frame with columns 'freqs' and 'probs'
if (!all(c("freqs", "probs") %in% names(weights))) {
stop("weights must be a data frame with columns 'freqs' and 'probs'")
}
# Sort weights by 'freqs' in ascending order
weights <- weights[order(weights$freqs), ]
cdf_probs <-c()
for (val in x){
#print(val)
# Initialize cumulative probability
cdf <- 0
# Loop through weights and calculate cumulative sum
for (i in 1:nrow(weights)) {
if (weights$freqs[i] <= val) {
cdf <- cdf + weights$probs[i]
} else {
cdf_probs <- c(cdf_probs,cdf)
break  # Stop iterating when freqs exceed x
}
}
#print("Precheck")
#print(weights$freqs[-1])
if (val >= tail(weights$freqs,n=1)){
cdf_probs <- c(cdf_probs,1)
}
}
return(cdf_probs)
}
max_read_depth <- 500
peak_probs_list <- list("0.25_0.75"= c(0.25,0.75),"0.5_0.5" = c(0.5,0.5))
base_probability_test <- compute_base_probability_table(max_read_depth,peak_probs_list)
read_depth_distr <- ceiling(rexp(1000,1/4))
central_prop <- 0.3
final_pmf_weights_test <- compute_pmf_probs(base_probability_test, read_depth_distr, central_prop)
sum(final_pmf_weights_test$probs)
read_depth_distr <- ceiling(rexp(1000,1/4))
#read_depth_distr <- round(rnorm(1000,30,2))
central_prop <- 0.3
final_pmf_weights_test <- compute_pmf_probs(base_probability_test, read_depth_distr, central_prop)
library(ggplot2)
# Plot the distributions
ggplot(final_pmf_weights_test, aes(x = freqs, y = probs)) +
geom_point() +  # Line for first distribution
labs(title = "Binomial Distributions",
x = "Number of Successes",
y = "Probability") +
xlim(-1,1) +
ylim(0,0.3)
View(as.data.frame(table(read_depth_distr)))
library(ggplot2)
read_depth <- 10
peak_probs <- c(0.5,0.5)
test_df_binom <- restricted_freq_distr(read_depth, peak_probs)
# Plot the distributions
ggplot(test_df_binom, aes(x = freqs, y = probs)) +
geom_point() +  # Line for first distribution
labs(title = "Binomial Distributions",
x = "Number of Successes",
y = "Probability") +
xlim(-1,1) +
ylim(0,0.5) +
#scale_x_continuous(breaks = seq(0, max(c(n1, n2)), 5)) +  # Adjust x-axis breaks for clarity
theme_bw()  # Use black and white theme
restricted_freq_distr <- function(read_depth, peak_probs){
reads_a <- seq(0,read_depth,1)
reads_b <- seq(read_depth,0,-1)
# Strong future optimization of this double binom estimation
read_binom_probs_a <- dbinom(reads_a,size = read_depth, prob = peak_probs[1])
read_binom_probs_b <- dbinom(reads_a,size = read_depth, prob = peak_probs[2])
final_freqs <- (reads_a - reads_b) / read_depth
final_freqs_probs <- ( read_binom_probs_a + read_binom_probs_b ) / 2
final_df <- data.frame(freqs = final_freqs, probs = final_freqs_probs)
return(final_df)
}
library(ggplot2)
read_depth <- 10
peak_probs <- c(0.5,0.5)
test_df_binom <- restricted_freq_distr(read_depth, peak_probs)
# Plot the distributions
ggplot(test_df_binom, aes(x = freqs, y = probs)) +
geom_point() +  # Line for first distribution
labs(title = "Binomial Distributions",
x = "Number of Successes",
y = "Probability") +
xlim(-1,1) +
ylim(0,0.5) +
#scale_x_continuous(breaks = seq(0, max(c(n1, n2)), 5)) +  # Adjust x-axis breaks for clarity
theme_bw()  # Use black and white theme
eiling(rexp(10,1/4))
ceiling(rexp(10,1/4))
rexp(10,1/4)
ceiling(rexp(10,1/4))
ceiling(rexp(10,1/4))
ceiling(rexp(10,1/4))
ceiling(rexp(10,1/4))
ceiling(rexp(10,1/4))
ceiling(rexp(10,1/4))
rpois(10,4)
rpois(20,4)
rpois(20,20)
as.data.frame(table(rpois(20,20)))
as.data.frame(table(rpois(1000,4)))
sum(as.data.frame(table(rpois(1000,4)))$Freq)
as.data.frame(table(rpois(1000,4)))
as.data.frame(table(rpois(100000,4)))
as.data.frame(table(rpois(1000000,4)))
compute_base_probability_table <- function(max_read_depth,peak_probs_list){
probability_table <- data.frame()
for (read_depth in c(2:max_read_depth)){
for (peak_prob_name in names(peak_probs_list)){
data_r <-restricted_freq_distr_viable(read_depth, peak_probs_list[[peak_prob_name]])
data_r$peak_type <- peak_prob_name
data_r$read_depth <- read_depth
probability_table <- rbind(probability_table, data_r)
}
}
return(probability_table)
}
restricted_freq_distr_viable <- function(read_depth, peak_probs){
reads_a <- seq(1,read_depth-1,1)
reads_b <- seq(read_depth-1,1,-1)
# Strong future optimization of this double binom estimation
read_binom_probs_a <- dbinom(reads_a,size = read_depth, prob = peak_probs[1])
read_binom_probs_b <- dbinom(reads_a,size = read_depth, prob = peak_probs[2])
final_freqs <- (reads_a - reads_b) / read_depth
final_freqs_probs <- (read_binom_probs_a + read_binom_probs_b ) / (2*sum(read_binom_probs_a))
final_df <- data.frame(freqs = final_freqs, probs = final_freqs_probs)
return(final_df)
}
compute_pmf_probs <- function(base_prob_table, read_depth_distr, central_prop){
read_depth_table <- as.data.frame(table(read_depth_distr))
out_ward_prop <- (1-central_prop)
filtered_base_table <- base_prob_table[base_prob_table$read_depth %in% read_depth_table$read_depth_distr,]
filtered_base_table[filtered_base_table$peak_type == "0.5_0.5",c("probs")] <- filtered_base_table[filtered_base_table$peak_type == "0.5_0.5",c("probs")] * central_prop
filtered_base_table[filtered_base_table$peak_type == "0.25_0.75",c("probs")] <- filtered_base_table[filtered_base_table$peak_type == "0.25_0.75",c("probs")] * out_ward_prop
final_weights <- aggregate(probs ~ freqs, filtered_base_table, FUN = sum)
# I rest 1 at the end because read depths of 1 get ignored.....
final_weights$probs <- final_weights$probs / (length(read_depth_table$read_depth_distr) - 1)
return(final_weights)
}
simple_pmf_function <- function(x, final_pmf_weights){
if (x %in% final_pmf_weights$freqs){
return(final_pmf_weights[final_pmf_weights$freqs == x, c("probs")])
}else{
return(0)
}
}
simple_cdf_func <- function(x,weights) {
# Check if weights is a data frame with columns 'freqs' and 'probs'
if (!all(c("freqs", "probs") %in% names(weights))) {
stop("weights must be a data frame with columns 'freqs' and 'probs'")
}
# Sort weights by 'freqs' in ascending order
weights <- weights[order(weights$freqs), ]
cdf_probs <-c()
for (val in x){
#print(val)
# Initialize cumulative probability
cdf <- 0
# Loop through weights and calculate cumulative sum
for (i in 1:nrow(weights)) {
if (weights$freqs[i] <= val) {
cdf <- cdf + weights$probs[i]
} else {
cdf_probs <- c(cdf_probs,cdf)
break  # Stop iterating when freqs exceed x
}
}
#print("Precheck")
#print(weights$freqs[-1])
if (val >= tail(weights$freqs,n=1)){
cdf_probs <- c(cdf_probs,1)
}
}
return(cdf_probs)
}
max_read_depth <- 500
peak_probs_list <- list("0.25_0.75"= c(0.25,0.75),"0.5_0.5" = c(0.5,0.5))
base_probability_test <- compute_base_probability_table(max_read_depth,peak_probs_list)
read_depth_distr <- ceiling(rexp(1000,1/4))
central_prop <- 0.3
final_pmf_weights_test <- compute_pmf_probs(base_probability_test, read_depth_distr, central_prop)
sum(final_pmf_weights_test$probs)
read_depth_distr <- ceiling(rexp(1000,1/4))
#read_depth_distr <- round(rnorm(1000,30,2))
central_prop <- 0.3
final_pmf_weights_test <- compute_pmf_probs(base_probability_test, read_depth_distr, central_prop)
library(ggplot2)
# Plot the distributions
ggplot(final_pmf_weights_test, aes(x = freqs, y = probs)) +
geom_point() +  # Line for first distribution
labs(title = "Binomial Distributions",
x = "Number of Successes",
y = "Probability") +
xlim(-1,1) +
ylim(0,0.3)
simple_pmf_function(0.5,final_pmf_weights_test)
simple_cdf_func(c(1,1,1,1),final_pmf_weights_test)
simple_cdf_func(c(-0.5,0,0.5),final_pmf_weights_test)
as.data.frame(table(ceiling(rexp(1000,1/4))))
as.data.frame(table(ceiling(rexp(1000,1/20))))
as.data.frame(table(ceiling(rexp(1000,1/10))))
seq(1,10)
seq(1,10,2)
seq(2,10)
max_depth <- 200
average_pos <- 5
read_depth_pos <- seq(2,max_depth)
pois_distr <- dpois(read_depth_pos,average_pos)
read_depth_pos
pois_distr
max_depth <- 200
average_pos <- 5
number_of_sites <- 1000
read_depth_pos <- seq(2,max_depth)
pois_distr <- dpois(read_depth_pos,average_pos)
read_depth_disrt_pois <- sample(read_depth_pos,number_of_sites,replace = TRUE, prob = pois_distr)
as.data.frame(table(read_depth_disrt_pois))
max_depth <- 200
average_exp <- 5
number_of_sites <- 1000
read_depth_exp <- seq(2,max_depth)
exp_distr <- dpois(read_depth_pos,1/average_exp)
exp_distr
max_depth <- 200
average_exp <- 5
number_of_sites <- 1000
read_depth_exp <- seq(2,max_depth)
exp_distr <- dpois(read_depth_exp,1/average_exp)
read_depth_disrt_exp <- sample(read_depth_exp,number_of_sites,replace = TRUE, prob = exp_distr)
as.data.frame(table(read_depth_disrt_exp))
max_depth <- 200
average_exp <- 10
number_of_sites <- 1000
read_depth_exp <- seq(2,max_depth)
exp_distr <- dpois(read_depth_exp,1/average_exp)
read_depth_disrt_exp <- sample(read_depth_exp,number_of_sites,replace = TRUE, prob = exp_distr)
as.data.frame(table(read_depth_disrt_exp))
read_depth_disrt_exp <- ceiling(rexp(1000,1/4)) + 1
as.data.frame(table(read_depth_disrt_exp))
read_depth_disrt_exp <- ceiling(rexp(1000,1/average_exp)) + 1
read_depth_disrt_exp <- ceiling(rexp(number_of_sites,1/average_exp)) + 1
as.data.frame(table(read_depth_disrt_exp))
max_depth <- 200
average_exp <- 4
number_of_sites <- 1000
read_depth_exp <- seq(2,max_depth)
exp_distr <- dpois(read_depth_exp,1/average_exp)
read_depth_disrt_exp <- sample(read_depth_exp,number_of_sites,replace = TRUE, prob = exp_distr)
read_depth_disrt_exp <- ceiling(rexp(number_of_sites,1/average_exp)) + 1
as.data.frame(table(read_depth_disrt_exp))
read_depth_distr_exp <- sample(read_depth_exp,number_of_sites,replace = TRUE, prob = exp_distr)
read_depth_distr_exp <- ceiling(rexp(number_of_sites,1/average_exp)) + 1
as.data.frame(table(read_depth_disrt_exp))
