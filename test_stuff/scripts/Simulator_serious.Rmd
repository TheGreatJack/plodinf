---
title: "simulator_plodinf"
author: "Florian"
date: "`r Sys.Date()`"
output: html_document
---


Here i would create code to simulate snp proportion distributions for differnt ploidies, sequencing parameters, and some biological cosiderations regarding SNPs. The idea is to be able to run many simulations with varying degrees of:

- ploidy
- read depth distribution
- mean read depth
- mean read error rate
- minimum allele depth / maximum too?
- number of SNP sites
- proportion of 0.5/0.5 to 0.25/0.75 sites


General idea:

I need to create a function that would take a read depth distribution. The function per se would be agnostic to the distribution itself. But a discrete exponential or discrete normal or poisson can be expected here. Then given a number of SNP sites to simulate the sites would be assigned to each read depth according to the read depth distribution. 

Now here things vary depending on the ploidy. In ploides 2n and 3n, there is only 1 possibility for biallelic SNPs. From 4n onwards there are multiple possibilities. Given the ploidy a proportion of sites should be allocated to each possibility. This proportion is parameter to simulate. In practice the model should be able to estimate this. 

Then for each read depth and for each possiblity of biallelic SNPs a sampling of the biallelic SNP proportions is made. X reads to one allele and Y reads to the other allele depending on the proportions. 

Errors from sequencing or even mapping can be varied. PCR errors, uncertainty of a basecall or erroneous mapping can cause: Biased allele proportions, SNP sites that are not real. False biallelic SNPs would probably show as sites with a predominant proportion of one allele, the error would have a low proportion. Those are the only sites that would be reasonably easy to model coming from read error rate. Fake biallelic snps would be a function of read depth distr, genome size and mean error rate of reads. 


Biased proportions from PCR errors would be difficult to assess in an empirical setting, it would only be seen as higher variance in the biallelic snp distribution. 

Error rate would be ignored from this simulator, as the effects are quite complex to model, in general should be ignorable. 


The first step towards this end is:

- Read depth distribution


# Read depth distr simulation

I guess poisson is the eassiest rightnow can be a simple poisson distribution. The rate would be defined as the average read depth. This should model ok the nature of read depth distributions. To control for low read depth, 1 or less i first generate the distribution from 2 to up to a max of 200 or so. And the i sample from that. 


```{r}
max_depth <- 200
average_pois <- 5
number_of_sites <- 1000

read_depth_pos <- seq(2,max_depth)
pois_distr <- dpois(read_depth_pos,average_pois)

read_depth_disrt_pois <- sample(read_depth_pos,number_of_sites,replace = TRUE, prob = pois_distr)
as.data.frame(table(read_depth_disrt_pois))


```

On the other hand a exponential is quite good. The poisson has not than high of a range


```{r}
max_depth <- 200
average_exp <- 4
number_of_sites <- 1000

read_depth_exp <- seq(2,max_depth)
exp_distr <- dexp(read_depth_exp,1/average_exp)


read_depth_distr_exp <- sample(read_depth_exp,number_of_sites,replace = TRUE, prob = exp_distr)
read_depth_distr_exp <- ceiling(rexp(number_of_sites,1/average_exp)) + 1

as.data.frame(table(read_depth_distr_exp))

```


# Simulator big itself

Take the read depth table, and iterate over each depth using the frequency as the number of snps. If tetraploid sample the number of central sites vs the outer sites using the central prop. Then for each site type (central and outer) and number (denominated as "c"), make "c" samplings of each proportion with the read depth as the sampling size for the sampling, estimate the porportions and save that to a dataframe. 



```{r}
snp_simulator_2n_4n <- function(current_ploidy, read_depth_table,central_prop) {
  
  
  # Define biallelic frequencies for different ploidies
  biallelic_freqs_diploid <- list(c(0.5, 0.5))
  biallelic_freqs_triploid <- list(c(0.33, 0.66))
  biallelic_freqs_tetraploid <- list(c(0.5,0.5),c(0.25, 0.75))
  

  # Get corresponding biallelic frequencies
  if (current_ploidy == 2) {
    biallelic_freqs <- biallelic_freqs_diploid
    all_props <- snp_gen_non_tetraploid(biallelic_freqs, read_depth_table)
  } else if (current_ploidy == 3) {
    biallelic_freqs <- biallelic_freqs_triploid
    all_props <- snp_gen_non_tetraploid(biallelic_freqs, read_depth_table)
  } else if (current_ploidy == 4){
    biallelic_freqs <- biallelic_freqs_tetraploid
    all_props <- snp_gen_tetraploid(biallelic_freqs, read_depth_table,central_prop)
  }
  colnames(all_props) <- c("read_depth","reads_a","reads_b","prop_a","prop_b")
  all_props$prop_diff <- all_props$prop_a - all_props$prop_b
  return(all_props)
}


snp_gen_non_tetraploid <- function(biallelic_freqs, read_depth_table) {
  # Initialize empty matrix to store allele proportions
  #all_props <- matrix(nrow = n_sites, ncol = 5)
  all_props <- data.frame()
  
  # Simulate for each SNP site
  for (i in 1:nrow(read_depth_table)) {
    read_depth <- read_depth_table[i,1]
    read_depth_freq <- read_depth_table[i,2]
    #print(biallelic_freqs[[1]])
    
    for (site in 1:read_depth_freq) {
      # Sample alleles based on frequencies
      alleles <- sample(c("A", "B"), size = read_depth, prob = biallelic_freqs[[1]], replace = TRUE)
      
      alleles_a <- sum(alleles == "A")
      alleles_b <- sum(alleles == "B")
      
      # Estimate allele proportions
      prop_a <- alleles_a / read_depth
      prop_b <- alleles_b / read_depth  
      
      all_props <- rbind(all_props,c(read_depth,alleles_a,alleles_b,prop_a, prop_b))
    }
  }
  # Return allele proportions
  return(all_props)
}


snp_gen_tetraploid <- function(biallelic_freqs, read_depth_table, central_prop) {
  # Initialize empty matrix to store allele proportions
  #all_props <- matrix(nrow = n_sites, ncol = 5)
  
  out_ward_prop <- (1-central_prop)
  all_props <- data.frame()
  
  # Simulate for each SNP site
  for (i in 1:nrow(read_depth_table)) {
    read_depth <- read_depth_table[i,1]
    read_depth_freq <- read_depth_table[i,2]
    
    biallelic_type_tbl <- table(sample(c("c", "o"), size = read_depth_freq, prob = c(central_prop,out_ward_prop), replace = TRUE))
    
    read_depth_c <- biallelic_type_tbl["c"]
    read_depth_o <- biallelic_type_tbl["o"]
    #print(biallelic_freqs[[1]])
    
    if (!is.na(read_depth_c)) {
      for (site in 1:read_depth_c) {
        # Sample alleles based on frequencies
        alleles <- sample(c("A", "B"), size = read_depth, prob = biallelic_freqs[[1]], replace = TRUE)
        
        alleles_a <- sum(alleles == "A")
        alleles_b <- sum(alleles == "B")
        
        # Estimate allele proportions
        prop_a <- alleles_a / read_depth
        prop_b <- alleles_b / read_depth  
        
        all_props <- rbind(all_props,c(read_depth,alleles_a,alleles_b,prop_a, prop_b))
      }
    }
    #print(!is.na(read_depth_o))
    if (!is.na(read_depth_o)) {
      for (site in 1:read_depth_o) {
        # Sample alleles based on frequencies
        alleles <- sample(c("A", "B"), size = read_depth, prob = biallelic_freqs[[2]], replace = TRUE)
        
        alleles_a <- sum(alleles == "A")
        alleles_b <- sum(alleles == "B")
        
        # Estimate allele proportions
        prop_a <- alleles_a / read_depth
        prop_b <- alleles_b / read_depth  
        
        all_props <- rbind(all_props,c(read_depth,alleles_a,alleles_b,prop_a, prop_b))
      }
    }
  }
  # Return allele proportions
  return(all_props)
}

```


## Testing 



```{r}
max_depth <- 200
average_pois <- 5
number_of_sites <- 1000

read_depth_pos <- seq(2,max_depth)
pois_distr <- dpois(read_depth_pos,average_pois)

read_depth_disrt_pois <- sample(read_depth_pos,number_of_sites,replace = TRUE, prob = pois_distr)
#as.data.frame(table(read_depth_disrt_pois))

pois_depth_distr <- as.data.frame(table(read_depth_disrt_pois))
pois_depth_distr$read_depth_disrt_pois <- as.numeric(as.character(pois_depth_distr$read_depth_disrt_pois))
pois_depth_distr
```


```{r}
current_ploidy <- 4
read_depth_table <- pois_depth_distr
central_prop <- 0.2

tetraplod_test <- snp_simulator_2n_4n(current_ploidy, read_depth_table,central_prop)

inversion_vector <- sample(c(-1,1), size = length(tetraplod_test$prop_diff),replace = TRUE)
tetraplod_test$prop_diff <- tetraplod_test$prop_diff * inversion_vector

tetraplod_test_table <- as.data.frame(table(tetraplod_test$prop_diff))
tetraplod_test_table$Var1 <- as.numeric(as.character(tetraplod_test_table$Var1))

tetraplod_test_table <- tetraplod_test_table[!(tetraplod_test_table$Var1 %in% c(-1,1)),]

tetraplod_test_table$probs <- tetraplod_test_table$Freq/sum(tetraplod_test_table$Freq)



```



```{r}

library(ggplot2)


# Plot the distributions
ggplot(tetraplod_test_table, aes(x = Var1, y = probs)) +
  geom_point(alpha = 0.5) +  # Line for first distribution  
  labs(title = "Binomial Distributions", 
       x = "Number of Successes", 
       y = "Probability") +
  xlim(-1,1) + 
  ylim(0,0.5) +
  #scale_x_continuous(breaks = seq(0, max(c(n1, n2)), 5)) +  # Adjust x-axis breaks for clarity
  theme_bw()  # Use black and white theme

```

The simulator works fine i guess i only have to ignore -1 and 1 sites and thats it.


# Fitting of distr to empirical data

I did a previous testing. I do not remember why but fitdistr of MASS just didn't work. In the end i used optim, that is also used within firdistr. I realized that i only have 1 variable to modify in my model. And that was only in the 4n and above case. Said variable was the central peak proportion. In the 2n and 3n is like the distribution is set.

Another problem i got is i think that of realized probabilities. In summary is that some read proportions although possible have very little probability of being observed. So in an empirical setting there is going to be zero observations of said proportions. My theory is that these missing proportions enhance the probability of already high probability read proportions. That "lost probability" must go somewhere. 

So i cannot fit an empirical distribution directly to the theorethical distribution. The empirical distribution should also be modeled by a factor that biases the read porportion probabilities given the number of reads sampled. Whats the expected empirical distribution of a theoretical distribution given a particular level of sampling?. At the end it has to do with the relationship between sampling and variance i guess. 

I shall replicate this problem and try to solve it by sampling the theorethical distribution then comparing it to an empirical simulation. 

After trying for a while i found out that i had a mistake in the function that was computing the final pmf probabilities, and also in the per read depth probabilities. I was assuming that every read depth was equally likely for some reason. Now the simulation and the theorethical look a bit more similar. It kinda makes me think that i should also include the "hidden" snps in the model. I'll test that. Also the model cant model 2n or 3n, i should all that too. 



## Binom funcs

```{r}
compute_base_probability_table <- function(max_read_depth,peak_probs_list){
  
  probability_table <- data.frame()
  
  for (read_depth in c(2:max_read_depth)){
    for (peak_prob_name in names(peak_probs_list)){
      data_r <-restricted_freq_distr_viable(read_depth, peak_probs_list[[peak_prob_name]])
      data_r$peak_type <- peak_prob_name
      #data_r$read_depth <- read_depth
      probability_table <- rbind(probability_table, data_r)
    }
  }
  return(probability_table)
}



restricted_freq_distr_viable <- function(read_depth, peak_probs){
  reads_a <- seq(1,read_depth-1,1)
  reads_b <- seq(read_depth-1,1,-1)
  
  # Strong future optimization of this double binom estimation 
  read_binom_probs_a <- dbinom(reads_a,size = read_depth, prob = peak_probs[1])
  read_binom_probs_b <- dbinom(reads_a,size = read_depth, prob = peak_probs[2])
  
  final_freqs <- (reads_a - reads_b) / read_depth
  
  final_freqs_probs <- (read_binom_probs_a + read_binom_probs_b ) / (sum(read_binom_probs_a,read_binom_probs_b))
  
  final_df <- data.frame(read_depth = read_depth, reads_a = reads_a , reads_b = reads_b,freqs = final_freqs, probs = final_freqs_probs)
  
  return(final_df)
  
}

compute_pmf_probs <- function(base_prob_table, read_depth_distr, central_prop){
  read_depth_table <- as.data.frame(table(read_depth_distr))
  read_depth_table$depth_prob <- read_depth_table$Freq / sum(read_depth_table$Freq) 
  
  out_ward_prop <- (1-central_prop)
  
  
  filtered_base_table <- base_prob_table[base_prob_table$read_depth %in% read_depth_table$read_depth_distr,]
  
  filtered_base_table[filtered_base_table$peak_type == "0.5_0.5",c("probs")] <- filtered_base_table[filtered_base_table$peak_type == "0.5_0.5",c("probs")] * central_prop

  filtered_base_table[filtered_base_table$peak_type == "0.25_0.75",c("probs")] <- filtered_base_table[filtered_base_table$peak_type == "0.25_0.75",c("probs")] * out_ward_prop
  
  
  filtered_base_table <- merge(filtered_base_table, read_depth_table, by.x = "read_depth", by.y = "read_depth_distr", all.x = TRUE)
  
  filtered_base_table$corrected_probs <- filtered_base_table$probs * filtered_base_table$depth_prob
  
  final_weights <- aggregate(corrected_probs ~ freqs, filtered_base_table, FUN = sum)
  
  # I rest 1 at the end because read depths of 1 get ignored.....
  #final_weights$probs <- final_weights$probs / (length(read_depth_table$read_depth_distr))
  
  return(final_weights)
}

simple_pmf_function <- function(x, final_pmf_weights){
  if (x %in% final_pmf_weights$freqs){
    return(final_pmf_weights[final_pmf_weights$freqs == x, c("probs")])
  }else{
    return(0)
  }
}

modified_pmf_function <- 

simple_cdf_func <- function(x,weights) {
  # Check if weights is a data frame with columns 'freqs' and 'probs'
  if (!all(c("freqs", "probs") %in% names(weights))) {
    stop("weights must be a data frame with columns 'freqs' and 'probs'")
  }
  
  # Sort weights by 'freqs' in ascending order
  weights <- weights[order(weights$freqs), ]

  
  cdf_probs <-c()
  
  for (val in x){
    #print(val)
    # Initialize cumulative probability
    cdf <- 0
    # Loop through weights and calculate cumulative sum
    for (i in 1:nrow(weights)) {
      if (weights$freqs[i] <= val) {
        cdf <- cdf + weights$probs[i]
      } else {
        cdf_probs <- c(cdf_probs,cdf)
        break  # Stop iterating when freqs exceed x
      }
    }
    #print("Precheck")
    #print(weights$freqs[-1])
    if (val >= tail(weights$freqs,n=1)){
      cdf_probs <- c(cdf_probs,1)       
    }
  }
  return(cdf_probs)
}
```

### Testing


```{r}
max_read_depth <- 500
peak_probs_list <- list("0.25_0.75"= c(0.25,0.75),"0.5_0.5" = c(0.5,0.5))

base_probability_test <- compute_base_probability_table(max_read_depth,peak_probs_list)
```


```{r}
#read_depth_distr <- ceiling(rexp(1000,1/4)) + 1
#read_depth_distr <- read_depth_disrt_pois
read_depth_distr <- tetraplod_test[!(tetraplod_test$prop_diff %in% c(-1,1)),"read_depth"]

central_prop <- 0.2

final_pmf_weights_test <- compute_pmf_probs(base_probability_test, read_depth_distr, central_prop)
sum(final_pmf_weights_test$corrected_probs)
```


```{r}
# Plot the distributions
ggplot(final_pmf_weights_test, aes(x = freqs, y = corrected_probs)) +
  geom_point(alpha = 0.5) +  # Line for first distribution  
  labs(title = "Binomial Distributions", 
       x = "Number of Successes", 
       y = "Probability") +
  xlim(-1,1) + 
  ylim(0,0.5) +
  #scale_x_continuous(breaks = seq(0, max(c(n1, n2)), 5)) +  # Adjust x-axis breaks for clarity
  theme_bw()  # U
```

A simple way to test my previous suspicion is sampling from a theoretical distribution and check if the sampling is too different from the observed empirical. 


```{r}
size <- 772

#sampled_freqs <- c(final_pmf_weights_test$freqs,-1,1)
#sampled_probs <- c((final_pmf_weights_test$corrected_probs)*0.8,0.1,0.1)

sampled_freqs <- c(final_pmf_weights_test$freqs)
sampled_probs <- c((final_pmf_weights_test$corrected_probs))


sample_from_theo <- sample(sampled_freqs, size = size, replace = TRUE, prob = sampled_probs)

sample_from_theo_tbl <- as.data.frame(table(sample_from_theo))
sample_from_theo_tbl$probs <- sample_from_theo_tbl$Freq/sum(sample_from_theo_tbl$Freq)

sample_from_theo_tbl$freqs <- as.numeric(as.character(sample_from_theo_tbl$sample_from_theo))

```

```{r}
# Plot the distributions
ggplot(sample_from_theo_tbl, aes(x = freqs, y = probs)) +
  geom_point(alpha = 0.5) +  # Line for first distribution  
  labs(title = "Binomial Distributions", 
       x = "Number of Successes", 
       y = "Probability") +
  xlim(-1,1) + 
  ylim(0,0.5) +
  #scale_x_continuous(breaks = seq(0, max(c(n1, n2)), 5)) +  # Adjust x-axis breaks for clarity
  theme_bw()  # U
```
Not many differences between the theory and the sampling... something is off. 

```{r}
# Plot the distributions
ggplot(tetraplod_test_table, aes(x = Var1, y = probs)) +
  geom_point(alpha = 0.5) +  # Line for first distribution  
  labs(title = "Binomial Distributions", 
       x = "Number of Successes", 
       y = "Probability") +
  xlim(-1,1) + 
  ylim(0,0.5) +
  #scale_x_continuous(breaks = seq(0, max(c(n1, n2)), 5)) +  # Adjust x-axis breaks for clarity
  theme_bw()  # Use black and white theme

```

I will compare now the simulation with the theoretical in a more straightforward way. 





```{r}
read_depth <- 10
reads_a <- seq(1,read_depth-1,1)
reads_b <- seq(read_depth-1,1,-1)

# Strong future optimization of this double binom estimation 
read_binom_probs_a <- dbinom(reads_a,size = read_depth, prob = c(0.25,0.75))
read_binom_probs_b <- dbinom(reads_a,size = read_depth, prob = c(0.5,0.5))

sum(read_binom_probs_a)
sum(read_binom_probs_b)
```

